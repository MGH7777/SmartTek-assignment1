import numpy as np
from typing import Dict, List, Tuple, Optional
import time
from collections import defaultdict
import cv2
from dataclasses import dataclass
import matplotlib.pyplot as plt
import csv
import os

@dataclass
class ContextModel:
    """CABAC Context Model with state adaptation"""
    state: int = 0
    mps: int = 0
    usage_count: int = 0
    lps_count: int = 0
    mps_flip_count: int = 0
    
    def update(self, bin_val: int):
        """Update context state with detailed tracking"""
        old_mps = self.mps
        
        # Complete state transition table
        transition_table = {
            0: (0, 1), 1: (0, 2), 2: (1, 3), 3: (2, 4), 4: (3, 5), 5: (4, 6),
            6: (5, 7), 7: (6, 8), 8: (7, 9), 9: (8, 10), 10: (9, 11), 11: (10, 12),
            12: (11, 13), 13: (12, 14), 14: (13, 15), 15: (14, 16), 16: (15, 17),
            17: (16, 18), 18: (17, 19), 19: (18, 20), 20: (19, 21), 21: (20, 22),
            22: (21, 23), 23: (22, 24), 24: (23, 25), 25: (24, 26), 26: (25, 27),
            27: (26, 28), 28: (27, 29), 29: (28, 30), 30: (29, 31), 31: (30, 32),
            32: (31, 33), 33: (32, 34), 34: (33, 35), 35: (34, 36), 36: (35, 37),
            37: (36, 38), 38: (37, 39), 39: (38, 40), 40: (39, 41), 41: (40, 42),
            42: (41, 43), 43: (42, 44), 44: (43, 45), 45: (44, 46), 46: (45, 47),
            47: (46, 48), 48: (47, 49), 49: (48, 50), 50: (49, 51), 51: (50, 52),
            52: (51, 53), 53: (52, 54), 54: (53, 55), 55: (54, 56), 56: (55, 57),
            57: (56, 58), 58: (57, 59), 59: (58, 60), 60: (59, 61), 61: (60, 62),
            62: (61, 62), 63: (63, 63)
        }
        
        if bin_val == self.mps:
            self.state = transition_table[self.state][0]
        else:
            self.state = transition_table[self.state][1]
            self.lps_count += 1
            
            # Track MPS flips
            if self.state == 0:
                self.mps = 1 - self.mps
                self.mps_flip_count += 1
        
        self.usage_count += 1

class OptimizedCABAC:
    """Base CABAC implementation with content-specific optimizations"""
    
    def __init__(self, content_type: str = "generic"):
        self.content_type = content_type
        self.context_models = {}
        self.initialize_context_models()
        
    def initialize_context_models(self):
        """Initialize context models based on content type - FINAL OPTIMIZED"""
        if self.content_type == "screen_recording":
            self.context_models = {i: ContextModel(state=40, mps=0) for i in range(128)}
        elif self.content_type == "surveillance":
            self.context_models = {i: ContextModel(state=48, mps=0) for i in range(128)}
        else:
            self.context_models = {i: ContextModel(state=32, mps=0) for i in range(96)}
    
    def get_context_id(self, bin_val: int, position: Tuple[int, int], 
                    neighbor_bits: List[int]) -> int:
        """Get context ID based on spatial information - OPTIMAL COLOR-AWARE VERSION"""
        if self.content_type == "screen_recording":
            # Optimized for screen content (text/UI)
            x, y = position
            ctx = (x % 6) * 6 + (y % 6)  # Balanced spatial grouping
            if len(neighbor_bits) >= 1:
                ctx += neighbor_bits[0] * 36  # Previous bit only
            ctx += bin_val * 72
            ctx = ctx % len(self.context_models)
            
        elif self.content_type == "surveillance":
            # Color-aware surveillance context modeling
            x, y = position
            
            # Base spatial context with finer granularity for complex scenes
            ctx = (x % 10) * 10 + (y % 10)  # 100 spatial contexts
            
            # Enhanced neighbor influence for motion patterns
            if len(neighbor_bits) >= 2:
                pattern = (neighbor_bits[0] << 1) | neighbor_bits[1]
                ctx += pattern * 100  # Motion patterns
            
            # Add residual magnitude context (important for surveillance)
            if len(neighbor_bits) >= 3:
                # Use magnitude patterns from previous bits
                magnitude_pattern = neighbor_bits[2] * 400
                ctx += magnitude_pattern
            
            ctx += bin_val * 800  # Current bit influence
            ctx = ctx % len(self.context_models)
            
        elif self.content_type == "animation":
            # Balanced for mixed patterns
            ctx = (position[0] % 6) * 6 + (position[1] % 6)
            if neighbor_bits:
                ctx += neighbor_bits[0] * 36
            ctx = ctx % len(self.context_models)
        
        else:  # generic
            ctx = (position[0] % 6) * 6 + (position[1] % 6)
            if neighbor_bits:
                ctx += neighbor_bits[0] * 36
            ctx = ctx % len(self.context_models)
        
        return ctx
    
    def state_to_probability(self, state: int) -> float:
        """Convert context state to LPS probability"""
        alpha = 0.95
        p_lps = alpha ** (63 - state) * 0.5
        return max(0.001, min(0.5, p_lps))
    
    def bits_to_bytes(self, bits: List[str]) -> bytes:
        """Convert bit list to bytes"""
        bytes_list = []
        current_byte = 0
        bit_count = 0
        
        for bit in bits:
            current_byte = (current_byte << 1) | int(bit)
            bit_count += 1
            
            if bit_count == 8:
                bytes_list.append(current_byte)
                current_byte = 0
                bit_count = 0
        
        if bit_count > 0:
            current_byte <<= (8 - bit_count)
            bytes_list.append(current_byte)
        
        return bytes(bytes_list)

class EnhancedCABAC(OptimizedCABAC):
    """Enhanced CABAC with detailed statistics tracking"""
    
    def __init__(self, content_type: str = "generic", persist_context: bool = True):
        super().__init__(content_type)
        self.persist_context = persist_context
        self.frame_stats = []
        
    def encode_binary(self, binary_data: List[int], 
                    spatial_info: Optional[List[Tuple[int, int]]] = None,
                    frame_idx: int = 0) -> bytes:
        """Enhanced encoding with detailed statistics"""
        
        frame_start_time = time.time()
        frame_stats = {
            'frame_idx': frame_idx,
            'bins_encoded': len(binary_data),
            'lps_events': 0,
            'mps_flips': 0,
            'context_usage': defaultdict(int),
            'start_time': frame_start_time
        }
        
        # CABAC encoding state
        low = 0
        high = 0xFFFFFFFF
        bits_out = []
        pending_bits = 0
        
        for i, bin_val in enumerate(binary_data):
            # Get spatial information
            if spatial_info and i < len(spatial_info):
                position = spatial_info[i]
                neighbor_bits = []
                if i > 0:
                    neighbor_bits.append(binary_data[i-1])
                if i > 1:
                    neighbor_bits.append(binary_data[i-2])
            else:
                position = (i % 16, i // 16)
                neighbor_bits = []
            
            # Get context model
            ctx_id = self.get_context_id(bin_val, position, neighbor_bits)
            ctx_model = self.context_models[ctx_id]
            
            # Track before update
            old_mps = ctx_model.mps
            
            # Enhanced tracking
            frame_stats['context_usage'][ctx_id] += 1
            if bin_val != ctx_model.mps:
                frame_stats['lps_events'] += 1
            
            # Get probability and encode
            p_lps = self.state_to_probability(ctx_model.state)
            range_width = high - low + 1
            lps_range = int(range_width * p_lps)
            mps_range = range_width - lps_range
            
            if bin_val == ctx_model.mps:
                high = low + mps_range - 1
            else:
                low = low + mps_range
            
            # Update context model
            ctx_model.update(bin_val)
            
            # Track MPS flips after update
            if ctx_model.mps != old_mps:
                frame_stats['mps_flips'] += 1
            
            # Renormalization with safety counter to prevent infinite loops
            max_renorm_iterations = 1000  # Safety limit
            iterations = 0
            
            while (high - low) < 0x40000000 and iterations < max_renorm_iterations:
                iterations += 1
                if high < 0x80000000:
                    bits_out.append('0')
                    for _ in range(pending_bits):
                        bits_out.append('1')
                    pending_bits = 0
                elif low >= 0x80000000:
                    bits_out.append('1')
                    for _ in range(pending_bits):
                        bits_out.append('0')
                    pending_bits = 0
                    low -= 0x80000000
                    high -= 0x80000000
                else:
                    pending_bits += 1
                    low -= 0x40000000
                    high -= 0x40000000
                
                low = (low << 1) & 0xFFFFFFFF
                high = ((high << 1) | 1) & 0xFFFFFFFF
            
            # Emergency break if stuck in renormalization
            if iterations >= max_renorm_iterations:
                print(f"‚ö†Ô∏è  Renormalization stuck after {max_renorm_iterations} iterations at bin {i}")
                if pending_bits > 0:
                    if low < 0x40000000:
                        bits_out.append('0')
                        for _ in range(pending_bits):
                            bits_out.append('1')
                    else:
                        bits_out.append('1')
                        for _ in range(pending_bits):
                            bits_out.append('0')
                    pending_bits = 0
                # Reset range to continue encoding
                low = 0
                high = 0xFFFFFFFF
        
        # Finalization
        if pending_bits > 0:
            if low < 0x40000000:
                bits_out.append('0')
                for _ in range(pending_bits):
                    bits_out.append('1')
            else:
                bits_out.append('1')
                for _ in range(pending_bits):
                    bits_out.append('0')
        
        # Convert to bytes
        encoded_data = self.bits_to_bytes(bits_out)
        
        # Calculate frame statistics
        frame_stats['encode_time'] = time.time() - frame_start_time
        frame_stats['avg_context_state'] = np.mean([ctx.state for ctx in self.context_models.values()])
        frame_stats['compression_ratio'] = len(binary_data) / (len(encoded_data) * 8) if encoded_data else 0
        
        self.frame_stats.append(frame_stats)
        
        return encoded_data

class VideoFrameProcessor:
    """Processes video frames for CABAC encoding"""
    
    @staticmethod
    def extract_residuals(frame: np.ndarray, prev_frame: np.ndarray = None) -> np.ndarray:
        """Extract residual frame"""
        if prev_frame is None:
            return frame.astype(np.int16)
        
        if frame.shape != prev_frame.shape:
            prev_frame = cv2.resize(prev_frame, (frame.shape[1], frame.shape[0]))
        
        return (frame.astype(np.int16) - prev_frame.astype(np.int16))
    
    @staticmethod
    def residuals_to_binary(residuals: np.ndarray) -> Tuple[List[int], List[Tuple[int, int]]]:
        """Convert residuals to binary representation - OPTIMIZED VERSION"""
        binary_data = []
        spatial_info = []
        
        # Get frame dimensions
        height, width = residuals.shape
        
        skip_factor = 4
        
        for y in range(0, height, skip_factor):
            for x in range(0, width, skip_factor):
                residual = residuals[y, x]
                
                # Encode sign and magnitude
                sign_bit = 1 if residual < 0 else 0
                binary_data.append(sign_bit)
                spatial_info.append((x, y))
                
                # ‚ö° ONLY encode 4-bit magnitude instead of 8-bit for speed
                magnitude = min(abs(residual), 15)  # Cap at 4 bits (0-15)
                for bit_pos in range(4):  # Only 4 bits instead of 8
                    bit = (magnitude >> bit_pos) & 1
                    binary_data.append(bit)
                    spatial_info.append((x, y))
        
        original_pixels = height * width
        processed_pixels = (height // skip_factor) * (width // skip_factor)
        print(f"‚ö° Optimized: Processed {processed_pixels}/{original_pixels} pixels "
            f"({processed_pixels/original_pixels*100:.1f}% of frame)")
        
        return binary_data, spatial_info
    

class DualModeCABACAnalyzer:
    """Analyzer that supports both real video files and synthetic data"""
    
    def __init__(self):
        self.content_types = ["generic", "screen_recording", "surveillance", "animation"]
        self.results = {}
        self.detailed_results = []
    
    def analyze_real_video(self, video_path: str, content_type: str, max_frames: int = 10, persist_context: bool = True):
        """Analyze real video file with specified content type - COLOR-AWARE VERSION"""
        print(f"üé¨ Analyzing REAL video: {video_path}")
        print(f"üìä Content type: {content_type}, Max frames: {max_frames}")
        
        if not os.path.exists(video_path):
            print(f"‚ùå Video file not found: {video_path}")
            return None
        
        cap = cv2.VideoCapture(video_path)
        frame_count = min(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), max_frames)
        
        if frame_count == 0:
            print("‚ùå No frames found in video")
            return None
        
        # Initialize CABAC encoders for all content types
        cabac_encoders = {
            ctype: EnhancedCABAC(ctype, persist_context) 
            for ctype in self.content_types
        }
        
        compression_ratios = {ctype: [] for ctype in self.content_types}
        adaptation_speeds = {ctype: [] for ctype in self.content_types}
        frame_results = []
        
        prev_frame = None
        for frame_idx in range(frame_count):
            ret, frame = cap.read()
            if not ret:
                break
                
            print(f"üì∑ Processing frame {frame_idx + 1}/{frame_count}")
            frame_start = time.time()
            
            if len(frame.shape) == 3:
                frame_yuv = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)
                frame = frame_yuv[:,:,0] 
                print("   üé® Processing luminance channel (Y) from YUV color space")
            
            residual_start = time.time()
            residuals = VideoFrameProcessor.extract_residuals(frame, prev_frame)
            residual_time = time.time() - residual_start
            print(f"   Residuals: {residual_time:.2f}s")
            
            binary_start = time.time()
            binary_data, spatial_info = VideoFrameProcessor.residuals_to_binary(residuals)
            binary_time = time.time() - binary_start
            print(f"   Binary conversion: {binary_time:.2f}s, Data size: {len(binary_data)} bits")
            
            prev_frame = frame
            
            frame_result = {'frame_idx': frame_idx, 'video_path': video_path, 'detected_type': content_type}
            
            for ctype, encoder in cabac_encoders.items():
                encode_start = time.time()
                encoded_data = encoder.encode_binary(binary_data, spatial_info, frame_idx)
                encode_time = time.time() - encode_start
                print(f"   {ctype} encoding: {encode_time:.2f}s")
                
                if encoder.frame_stats:
                    stats = encoder.frame_stats[-1]
                    
                    frame_result[ctype] = {
                        'compression_ratio': stats['compression_ratio'],
                        'encode_time': stats['encode_time'],
                        'avg_context_state': stats['avg_context_state'],
                        'lps_rate': stats['lps_events'] / len(binary_data) if binary_data else 0,
                        'bits_per_bin': (len(encoded_data) * 8) / len(binary_data) if binary_data else 0,
                    }
                    
                    compression_ratios[ctype].append(stats['compression_ratio'])
                    adaptation_speeds[ctype].append(stats['avg_context_state'])
            
            frame_results.append(frame_result)
            
            frame_total = time.time() - frame_start
            print(f"   Frame total: {frame_total:.2f}s")
            print(f"   Estimated remaining: {(frame_count - frame_idx - 1) * frame_total:.1f}s")
        
        cap.release()
        
        # Store results
        video_key = f"real_{os.path.basename(video_path)}_{content_type}"
        self.results[video_key] = {
            'detected_type': content_type,
            'compression_ratios': compression_ratios,
            'adaptation_speeds': adaptation_speeds,
            'frame_count': frame_count,
            'frame_results': frame_results
        }
        
        # Save detailed results
        try:
            self.save_real_video_results(video_path, content_type, frame_results)
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not save results file: {e}")
            print("üìä Analysis completed successfully, results available in memory")        
        
        # Analyze context models
        print(f"\nüî¨ Context Model Analysis for {content_type}:")
        self.analyze_context_models_comparison(cabac_encoders, content_type)
        
        return self.results[video_key]
    
    def run_synthetic_analysis(self, specialized_type: str = "screen_recording"):
        """Run synthetic analysis comparing generic vs specialized content"""
        print(f"üé¨ Running SYNTHETIC analysis: Generic vs {specialized_type}")
        print("=" * 60)
        
        # Create analyzers
        generic_cabac = EnhancedCABAC("generic")
        specialized_cabac = EnhancedCABAC(specialized_type)
        
        # Generate synthetic data matching the specialized content
        if specialized_type == "screen_recording":
            test_data = self.generate_screen_content_data()
            content_desc = "Screen Recordings (low entropy, predictable)"
        elif specialized_type == "surveillance":
            test_data = self.generate_surveillance_content_data() 
            content_desc = "Surveillance (high entropy, complex)"
        elif specialized_type == "animation":
            test_data = self.generate_animation_content_data()
            content_desc = "Animation (mixed patterns)"
        else:
            test_data = self.generate_generic_content_data()
            content_desc = "Generic Video"
        
        print(f"üìä Testing on: {content_desc}")
        
        spatial_info = [(i % 16, i // 16) for i in range(len(test_data))]
        
        # Encode with both CABAC variants
        generic_encoded = generic_cabac.encode_binary(test_data, spatial_info)
        specialized_encoded = specialized_cabac.encode_binary(test_data, spatial_info)
        
        # Display comparison
        if generic_cabac.frame_stats and specialized_cabac.frame_stats:
            generic_stats = generic_cabac.frame_stats[0]
            specialized_stats = specialized_cabac.frame_stats[0]
            
            print(f"\nüìà SYNTHETIC COMPARISON RESULTS:")
            print("=" * 60)
            print(f"{'Metric':<25} {'Generic':<12} {specialized_type:<12} {'Improvement':<12}")
            print("-" * 60)
            print(f"{'Compression Ratio':<25} {generic_stats['compression_ratio']:<12.2f} {specialized_stats['compression_ratio']:<12.2f} {((specialized_stats['compression_ratio']/generic_stats['compression_ratio'])-1)*100:>+10.1f}%")
            print(f"{'Avg Context State':<25} {generic_stats['avg_context_state']:<12.1f} {specialized_stats['avg_context_state']:<12.1f} {'N/A':>12}")
            print(f"{'LPS Rate':<25} {generic_stats['lps_events']/len(test_data):<12.3f} {specialized_stats['lps_events']/len(test_data):<12.3f} {'N/A':>12}")
        
        # Context analysis
        print(f"\nüî¨ CONTEXT MODEL COMPARISON:")
        print("=" * 60)
        
        print(f"\nüìä Generic CABAC - Top 5 Contexts:")
        self.analyze_specific_contexts(generic_cabac, top_n=5)
        
        print(f"\nüìä {specialized_type} CABAC - Top 5 Contexts:")
        self.analyze_specific_contexts(specialized_cabac, top_n=5)
        
        # Store synthetic results for plotting
        synthetic_key = f"synthetic_{specialized_type}"
        self.results[synthetic_key] = {
            'detected_type': specialized_type,
            'compression_ratios': {
                'generic': [generic_stats['compression_ratio']],
                specialized_type: [specialized_stats['compression_ratio']]
            },
            'adaptation_speeds': {
                'generic': [generic_stats['avg_context_state']],
                specialized_type: [specialized_stats['avg_context_state']]
            },
            'frame_count': 1
        }
        
        return self.results[synthetic_key]
    
    def generate_screen_content_data(self) -> List[int]:
        """Generate data mimicking screen recordings"""
        np.random.seed(42)
        data = []
        current_bit = 0
        for _ in range(1000):
            if np.random.random() < 0.8:  # Long runs for flat UI areas
                data.append(current_bit)
            else:
                current_bit = 1 - current_bit
                data.append(current_bit)
        return data
    
    def generate_surveillance_content_data(self) -> List[int]:
        """Generate data mimicking surveillance footage"""
        np.random.seed(42)
        return [np.random.randint(0, 2) for _ in range(1000)]
    
    def generate_animation_content_data(self) -> List[int]:
        """Generate data mimicking animation"""
        np.random.seed(42)
        data = []
        current_bit = 0
        for _ in range(1000):
            if np.random.random() < 0.6:  # Moderate runs for smooth gradients
                data.append(current_bit)
            else:
                current_bit = 1 - current_bit
                data.append(current_bit)
        return data
    
    def generate_generic_content_data(self) -> List[int]:
        """Generate generic video data"""
        np.random.seed(42)
        data = []
        current_bit = 0
        for _ in range(1000):
            if np.random.random() < 0.7:  # Balanced runs
                data.append(current_bit)
            else:
                current_bit = 1 - current_bit
                data.append(current_bit)
        return data
    
    def analyze_specific_contexts(self, cabac: EnhancedCABAC, top_n: int = 5):
        """Analyze specific context model usage"""
        usage_stats = []
        for ctx_id, ctx_model in cabac.context_models.items():
            if ctx_model.usage_count > 0:
                lps_rate = ctx_model.lps_count / ctx_model.usage_count
                usage_stats.append((ctx_id, ctx_model.usage_count, lps_rate, ctx_model.mps_flip_count))
        
        usage_stats.sort(key=lambda x: x[1], reverse=True)
        
        for ctx_id, usage, lps_rate, flips in usage_stats[:top_n]:
            certainty = "HIGH" if lps_rate < 0.3 else "MEDIUM" if lps_rate < 0.5 else "LOW"
            print(f"  Context {ctx_id:3d}: usage={usage:4d}, LPS-rate={lps_rate:.3f} ({certainty}), flips={flips}")
    
    def analyze_context_models_comparison(self, cabac_encoders: Dict, content_type: str):
        """Compare context models across different CABAC variants"""
        for ctype, encoder in cabac_encoders.items():
            print(f"\nüìä {ctype.upper()} - Context Usage Summary:")
            total_usage = sum(ctx.usage_count for ctx in encoder.context_models.values())
            active_contexts = sum(1 for ctx in encoder.context_models.values() if ctx.usage_count > 0)
            print(f"  Active contexts: {active_contexts}/{len(encoder.context_models)}")
            print(f"  Total context usage: {total_usage}")
    
    def save_real_video_results(self, video_path: str, content_type: str, frame_results: List):
        """Save real video analysis results with proper error handling"""
        try:
            base_name = os.path.basename(video_path).split('.')[0]
            safe_base_name = "".join(c for c in base_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
            csv_filename = f"real_analysis_{safe_base_name}_{content_type}.csv"
            
            save_path = csv_filename
            
            try:
                with open(save_path, 'w', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        'video', 'frame', 'content_type', 'compression_ratio', 
                        'encode_time_ms', 'avg_context_state', 'lps_rate', 'bits_per_bin'
                    ])
                    
                    for frame_result in frame_results:
                        for ctype, metrics in frame_result.items():
                            if ctype not in ['frame_idx', 'video_path', 'detected_type']:
                                writer.writerow([
                                    frame_result['video_path'],
                                    frame_result['frame_idx'],
                                    ctype,
                                    metrics['compression_ratio'],
                                    metrics['encode_time'] * 1000,
                                    metrics['avg_context_state'],
                                    metrics['lps_rate'],
                                    metrics['bits_per_bin']
                                ])
                
                print(f"üíæ Real video results saved to: {save_path}")
                
            except PermissionError:
                documents_path = os.path.expanduser("~/Documents")
                save_path = os.path.join(documents_path, csv_filename)
                
                with open(save_path, 'w', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        'video', 'frame', 'content_type', 'compression_ratio', 
                        'encode_time_ms', 'avg_context_state', 'lps_rate', 'bits_per_bin'
                    ])
                    
                    for frame_result in frame_results:
                        for ctype, metrics in frame_result.items():
                            if ctype not in ['frame_idx', 'video_path', 'detected_type']:
                                writer.writerow([
                                    frame_result['video_path'],
                                    frame_result['frame_idx'],
                                    ctype,
                                    metrics['compression_ratio'],
                                    metrics['encode_time'] * 1000,
                                    metrics['avg_context_state'],
                                    metrics['lps_rate'],
                                    metrics['bits_per_bin']
                                ])
                
                print(f"üíæ Real video results saved to: {save_path} (Documents folder)")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not save results to file: {e}")
            print("üìä Results are still available in memory for analysis")
    
    def generate_comprehensive_report(self):
        """Generate report covering both real and synthetic analyses"""
        print("\n" + "=" * 80)
        print("COMPREHENSIVE CABAC ANALYSIS REPORT")
        print("=" * 80)
        print("üìã Includes both real video analysis and synthetic comparisons")
        print("=" * 80)
        
        real_analyses = {k: v for k, v in self.results.items() if k.startswith('real_')}
        synthetic_analyses = {k: v for k, v in self.results.items() if k.startswith('synthetic_')}
        
        if real_analyses:
            print("\nüé¨ REAL VIDEO ANALYSIS RESULTS:")
            for video_key, result in real_analyses.items():
                print(f"\nüìπ {video_key}:")
                print(f"   Content type: {result['detected_type']}")
                print(f"   Frames analyzed: {result['frame_count']}")
                
                if 'generic' in result['compression_ratios'] and result['detected_type'] in result['compression_ratios']:
                    generic_ratios = result['compression_ratios']['generic']
                    specialized_ratios = result['compression_ratios'][result['detected_type']]
                    
                    # Calculate bits needed instead of ratio comparison
                    generic_bits_needed = []
                    specialized_bits_needed = []
                    
                    for i in range(len(generic_ratios)):
                        # Convert compression ratio to bits needed (lower is better)
                        if generic_ratios[i] > 0:
                            generic_bits = 1.0 / generic_ratios[i]  # Bits per original bit
                        else:
                            generic_bits = 1.0  # Default if ratio is invalid
                        
                        if specialized_ratios[i] > 0:
                            specialized_bits = 1.0 / specialized_ratios[i]
                        else:
                            specialized_bits = 1.0
                        
                        generic_bits_needed.append(generic_bits)
                        specialized_bits_needed.append(specialized_bits)
                    
                    # Calculate average bits needed
                    avg_generic_bits = np.mean(generic_bits_needed)
                    avg_specialized_bits = np.mean(specialized_bits_needed)
                    
                    # Calculate improvement (reduction in bits needed)
                    if avg_generic_bits > 0:
                        improvement = ((avg_generic_bits - avg_specialized_bits) / avg_generic_bits) * 100
                    else:
                        improvement = 0.0
                    
                    # Display compression ratios for context
                    avg_generic_ratio = np.mean(generic_ratios)
                    avg_specialized_ratio = np.mean(specialized_ratios)
                    
                    print(f"   Generic CABAC:     {avg_generic_ratio:.2f} compression ratio")
                    print(f"   {result['detected_type']} CABAC: {avg_specialized_ratio:.2f} compression ratio")
                    
                    if improvement > 0:
                        print(f"   üéØ Improvement:     {improvement:+.1f}% reduction in bits needed")
                    else:
                        print(f"   üìâ Degradation:     {improvement:+.1f}% increase in bits needed")
                    
                    # Show context usage comparison
                    print(f"   üî¨ Context usage:   Generic: {len([r for r in generic_ratios if r > 1])}/{len(generic_ratios)} frames effective")
                    print(f"                      {result['detected_type']}: {len([r for r in specialized_ratios if r > 1])}/{len(specialized_ratios)} frames effective")
        
        if synthetic_analyses:
            print("\nüî¨ SYNTHETIC ANALYSIS RESULTS:")
            for synthetic_key, result in synthetic_analyses.items():
                specialized_type = result['detected_type']
                if 'generic' in result['compression_ratios'] and specialized_type in result['compression_ratios']:
                    generic_ratio = result['compression_ratios']['generic'][0]
                    specialized_ratio = result['compression_ratios'][specialized_type][0]
                    
                    # Calculate bits-based improvement for synthetic too
                    generic_bits = 1.0 / generic_ratio if generic_ratio > 0 else 1.0
                    specialized_bits = 1.0 / specialized_ratio if specialized_ratio > 0 else 1.0
                    improvement = ((generic_bits - specialized_bits) / generic_bits) * 100
                    
                    print(f"   {specialized_type}: {improvement:+.1f}% improvement over generic")
                    

class CABACVisualizer:
    """Creates comprehensive charts for CABAC analysis results"""
    
    @staticmethod
    def create_compression_comparison_chart(results: dict, save_path: str = None):
        """Create compression ratio comparison chart"""
        plt.figure(figsize=(12, 8))
        
        # Extract data for plotting
        content_types = []
        compression_ratios = []
        
        for key, result in results.items():
            if 'compression_ratios' in result:
                for ctype, ratios in result['compression_ratios'].items():
                    if ratios:  # Only if we have data
                        avg_ratio = np.mean(ratios)
                        content_types.append(ctype)
                        compression_ratios.append(avg_ratio)
        
        if not content_types:
            print("‚ö†Ô∏è No compression ratio data found for chart")
            return
        
        # Create bar chart
        colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'lightpink']
        bars = plt.bar(content_types, compression_ratios, 
                      color=colors[:len(content_types)], 
                      edgecolor='black', alpha=0.7)
        
        # Add value labels on bars
        for bar, ratio in zip(bars, compression_ratios):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,
                    f'{ratio:.2f}', ha='center', va='bottom', fontweight='bold')
        
        plt.title('CABAC Compression Ratio Comparison by Content Type', fontsize=14, fontweight='bold')
        plt.xlabel('Content Type', fontweight='bold')
        plt.ylabel('Compression Ratio (Higher is Better)', fontweight='bold')
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"üíæ Chart saved to: {save_path}")
        
        plt.show()
    
    @staticmethod
    def create_adaptation_speed_chart(results: dict, save_path: str = None):
        """Create context model adaptation speed chart"""
        plt.figure(figsize=(12, 8))
        
        for key, result in results.items():
            if 'adaptation_speeds' in result and 'frame_count' in result:
                frame_count = result['frame_count']
                for ctype, speeds in result['adaptation_speeds'].items():
                    if speeds and frame_count > 0:
                        frames = list(range(min(frame_count, len(speeds))))
                        plt.plot(frames, speeds[:frame_count], 
                                label=f'{ctype}', marker='o', linewidth=2, markersize=4)
        
        plt.title('Context Model Adaptation Speed Over Frames', fontsize=14, fontweight='bold')
        plt.xlabel('Frame Number', fontweight='bold')
        plt.ylabel('Average Context State (Lower = Faster Adaptation)', fontweight='bold')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend()
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"üíæ Chart saved to: {save_path}")
        
        plt.show()
    
    @staticmethod
    def create_performance_comparison_chart(results: dict, save_path: str = None):
        """Create comprehensive performance comparison chart"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Chart 1: Compression ratios
        content_data = {}
        for key, result in results.items():
            if 'compression_ratios' in result:
                for ctype, ratios in result['compression_ratios'].items():
                    if ctype not in content_data:
                        content_data[ctype] = []
                    content_data[ctype].extend(ratios)
        
        # Calculate averages
        avg_ratios = {ctype: np.mean(ratios) for ctype, ratios in content_data.items() if ratios}
        
        if avg_ratios:
            colors1 = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
            bars1 = ax1.bar(avg_ratios.keys(), avg_ratios.values(), 
                           color=colors1[:len(avg_ratios)], alpha=0.7)
            
            for bar, ratio in zip(bars1, avg_ratios.values()):
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,
                        f'{ratio:.2f}', ha='center', va='bottom', fontweight='bold')
            
            ax1.set_title('Average Compression Ratios', fontweight='bold')
            ax1.set_ylabel('Compression Ratio', fontweight='bold')
            ax1.grid(axis='y', linestyle='--', alpha=0.7)
            ax1.tick_params(axis='x', rotation=45)
        
        # Chart 2: Adaptation speeds
        adaptation_data = {}
        for key, result in results.items():
            if 'adaptation_speeds' in result:
                for ctype, speeds in result['adaptation_speeds'].items():
                    if ctype not in adaptation_data:
                        adaptation_data[ctype] = []
                    adaptation_data[ctype].extend(speeds)
        
        avg_speeds = {ctype: np.mean(speeds) for ctype, speeds in adaptation_data.items() if speeds}
        
        if avg_speeds:
            colors2 = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
            bars2 = ax2.bar(avg_speeds.keys(), avg_speeds.values(), 
                           color=colors2[:len(avg_speeds)], alpha=0.7)
            
            for bar, speed in zip(bars2, avg_speeds.values()):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,
                        f'{speed:.1f}', ha='center', va='bottom', fontweight='bold')
            
            ax2.set_title('Average Context State (Adaptation Speed)', fontweight='bold')
            ax2.set_ylabel('Context State', fontweight='bold')
            ax2.grid(axis='y', linestyle='--', alpha=0.7)
            ax2.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"üíæ Performance chart saved to: {save_path}")
        
        plt.show()
    
    @staticmethod
    def create_detailed_frame_analysis(frame_results: list, save_path: str = None):
        """Create detailed frame-by-frame analysis chart"""
        if not frame_results:
            print("‚ö†Ô∏è No frame results data available")
            return
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # Extract data for each content type
        content_types = set()
        for frame_result in frame_results:
            for key in frame_result.keys():
                if key not in ['frame_idx', 'video_path', 'detected_type']:
                    content_types.add(key)
        
        content_types = list(content_types)
        
        # Chart 1: Compression ratios over frames
        frame_numbers = [fr['frame_idx'] for fr in frame_results]
        for ctype in content_types:
            ratios = []
            for fr in frame_results:
                if ctype in fr and 'compression_ratio' in fr[ctype]:
                    ratios.append(fr[ctype]['compression_ratio'])
                else:
                    ratios.append(0)
            ax1.plot(frame_numbers, ratios, label=ctype, marker='o', linewidth=2)
        
        ax1.set_title('Compression Ratio Evolution Over Frames', fontweight='bold')
        ax1.set_xlabel('Frame Number')
        ax1.set_ylabel('Compression Ratio')
        ax1.legend()
        ax1.grid(True, linestyle='--', alpha=0.7)
        
        for ctype in content_types:
            lps_rates = []
            for fr in frame_results:
                if ctype in fr and 'lps_rate' in fr[ctype]:
                    lps_rates.append(fr[ctype]['lps_rate'])
                else:
                    lps_rates.append(0)
            ax2.plot(frame_numbers, lps_rates, label=ctype, marker='s', linewidth=2)
        
        ax2.set_title('LPS Rate Evolution Over Frames', fontweight='bold')
        ax2.set_xlabel('Frame Number')
        ax2.set_ylabel('LPS Rate')
        ax2.legend()
        ax2.grid(True, linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"üíæ Frame analysis chart saved to: {save_path}")
        
        plt.show()
        
        
def main():
    print("üé¨ CABAC Video Content Analysis")
    print("=" * 50)

    # Initialize analyzer and visualizer
    analyzer = DualModeCABACAnalyzer()
    visualizer = CABACVisualizer()
    
    while True:
        print("\nChoose analysis mode:")
        print("1. Compare ALL CABAC algorithms")
        print("2. Use SPECIFIC algorithm (choose one)")  
        print("3. Run SYNTHETIC analysis")
        print("4. Generate charts from previous results")
        print("5. Exit")
        
        choice = input("\nEnter choice (1-5) [1]: ").strip() or "1"
        
        if choice == "1":
            analyzer = analyze_real_video_interactive()
            if analyzer and analyzer.results:
                print("\nüìä Generating analysis charts...")
                visualizer.create_compression_comparison_chart(analyzer.results)
                visualizer.create_adaptation_speed_chart(analyzer.results)
                visualizer.create_performance_comparison_chart(analyzer.results)
                
        elif choice == "2":
            analyzer = analyze_with_chosen_algorithm()
            if analyzer and analyzer.results:
                print("\nüìä Generating comparison charts...")
                visualizer.create_compression_comparison_chart(analyzer.results)
                
        elif choice == "3":
            analyzer = run_synthetic_analysis_interactive()
            if analyzer and analyzer.results:
                print("\nüìä Generating synthetic analysis charts...")
                visualizer.create_compression_comparison_chart(analyzer.results)
                visualizer.create_performance_comparison_chart(analyzer.results)
                
        elif choice == "4":
            if analyzer and analyzer.results:
                print("\nüìä Generating charts from previous analysis...")
                visualizer.create_compression_comparison_chart(analyzer.results)
                visualizer.create_adaptation_speed_chart(analyzer.results)
                visualizer.create_performance_comparison_chart(analyzer.results)
                
                for key, result in analyzer.results.items():
                    if 'frame_results' in result and result['frame_results']:
                        visualizer.create_detailed_frame_analysis(result['frame_results'])
                        break
            else:
                print("‚ùå No previous analysis results found. Please run an analysis first.")
                
        elif choice == "5":
            print("Goodbye! üëã")
            break
        else:
            print("‚ùå Invalid choice.")
            continue


def analyze_real_video_interactive():
    """Interactive real video analysis"""
    print("\nüé¨ REAL VIDEO ANALYSIS")
    print("-" * 30)
    
    # Get video path
    video_path = input("Enter path to video file: ").strip()
    if not video_path:
        print("‚ùå No path provided. Returning to main menu.")
        return
    
    if not os.path.exists(video_path):
        print(f"‚ùå File not found: {video_path}")
        return
    
    # Get content type
    print("\nChoose video content type:")
    print("1. Screen Recording (PowerPoint, code editors)")
    print("2. Surveillance Footage (security cameras)")
    print("3. Animation (cartoons, CGI)")
    print("4. Generic Video (regular videos)")
    
    content_choice = input("Enter choice (1-4) [1]: ").strip() or "1"
    content_map = {"1": "screen_recording", "2": "surveillance", "3": "animation", "4": "generic"}
    content_type = content_map.get(content_choice, "screen_recording")
    
    # Get frame count
    try:
        frames = int(input("Number of frames to analyze [5]: ").strip() or "5")
    except ValueError:
        frames = 5
        print("‚ö†Ô∏è  Using default: 5 frames")
    
    # Ask about context persistence
    persist = input("Persist context between frames? (y/n) [y]: ").strip().lower() or "y"
    persist_context = persist in ['y', 'yes']
    
    print(f"\nüöÄ Starting analysis...")
    print(f"üìπ Video: {video_path}")
    print(f"üéØ Content: {content_type}")
    print(f"üìä Frames: {frames}")
    print(f"üîÑ Persist context: {persist_context}")
    
    # Run analysis
    analyzer = DualModeCABACAnalyzer()
    results = analyzer.analyze_real_video(video_path, content_type, frames, persist_context)
    
    if results:
        print("\n‚úÖ Real video analysis completed!")
        analyzer.generate_comprehensive_report()
        return analyzer
    else:
        print("‚ùå Analysis failed.")
        return None     

def run_synthetic_analysis_interactive():
    """Interactive synthetic analysis"""
    print("\nüî¨ SYNTHETIC ANALYSIS")
    print("-" * 25)
    
    print("Choose content type to compare against generic:")
    print("1. Screen Recording (optimized for UI/text content)")
    print("2. Surveillance Footage (optimized for complex scenes)")
    print("3. Animation (optimized for cartoons/CGI)")
    print("4. Compare ALL types")
    
    choice = input("Enter choice (1-4) [1]: ").strip() or "1"
    
    analyzer = DualModeCABACAnalyzer()
    
    if choice == "1":
        analyzer.run_synthetic_analysis("screen_recording")
    elif choice == "2":
        analyzer.run_synthetic_analysis("surveillance")
    elif choice == "3":
        analyzer.run_synthetic_analysis("animation")
    elif choice == "4":
        print("\nüîÑ Running comparison for ALL content types...")
        for content_type in ['screen_recording', 'surveillance', 'animation']:
            analyzer.run_synthetic_analysis(content_type)
            print("\n" + "="*50)
    else:
        analyzer.run_synthetic_analysis("screen_recording")
    
    analyzer.generate_comprehensive_report()
    print("\n‚úÖ Synthetic analysis completed!")
    
def analyze_with_chosen_algorithm():
    """Analyze video with user-chosen CABAC algorithm"""
    print("\nüé¨ ANALYZE WITH CHOSEN ALGORITHM")
    print("-" * 35)
    
    # Get video path
    video_path = input("Enter path to video file: ").strip()
    if not video_path or not os.path.exists(video_path):
        print("‚ùå File not found.")
        return
    
    # Choose algorithm
    print("\nChoose CABAC algorithm:")
    print("1. Generic CABAC")
    print("2. Screen Recording Optimized")
    print("3. Surveillance Optimized") 
    print("4. Animation Optimized")
    
    algo_choice = input("Enter choice (1-4) [1]: ").strip() or "1"
    algo_map = {"1": "generic", "2": "screen_recording", "3": "surveillance", "4": "animation"}
    chosen_algo = algo_map.get(algo_choice, "generic")
    
    # Get frames
    try:
        frames = int(input("Number of frames [3]: ").strip() or "3")
    except ValueError:
        frames = 3
    
    print(f"\nüöÄ Analyzing with {chosen_algo} CABAC...")
    
    analyzer = DualModeCABACAnalyzer()
    
    cap = cv2.VideoCapture(video_path)
    frame_count = min(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), frames)
    
    generic_cabac = EnhancedCABAC("generic")
    specialized_cabac = EnhancedCABAC(chosen_algo)
    
    prev_frame = None
    generic_ratios = []
    specialized_ratios = []
    
    for frame_idx in range(frame_count):
        ret, frame = cap.read()
        if not ret:
            break
            
        print(f"üì∑ Frame {frame_idx + 1}/{frame_count}")
        
        # Convert to grayscale
        if len(frame.shape) == 3:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Process frame
        residuals = VideoFrameProcessor.extract_residuals(frame, prev_frame)
        binary_data, spatial_info = VideoFrameProcessor.residuals_to_binary(residuals)
        prev_frame = frame
        
        # Encode with both
        generic_encoded = generic_cabac.encode_binary(binary_data, spatial_info, frame_idx)
        specialized_encoded = specialized_cabac.encode_binary(binary_data, spatial_info, frame_idx)
        
        if generic_cabac.frame_stats and specialized_cabac.frame_stats:
            generic_stats = generic_cabac.frame_stats[-1]
            specialized_stats = specialized_cabac.frame_stats[-1]
            
            generic_ratios.append(generic_stats['compression_ratio'])
            specialized_ratios.append(specialized_stats['compression_ratio'])
            
            generic_bits = 1.0 / generic_stats['compression_ratio'] if generic_stats['compression_ratio'] > 0 else 1.0
            specialized_bits = 1.0 / specialized_stats['compression_ratio'] if specialized_stats['compression_ratio'] > 0 else 1.0
            improvement = ((generic_bits - specialized_bits) / generic_bits) * 100
            print(f"   {chosen_algo}: {improvement:+.1f}% vs generic")
    
    cap.release()
    
    if generic_ratios and specialized_ratios:
        avg_generic = np.mean(generic_ratios)
        avg_specialized = np.mean(specialized_ratios)
        avg_improvement = ((avg_specialized / avg_generic) - 1) * 100
        
        print(f"\nüìà FINAL RESULTS:")
        print(f"   Generic CABAC:     {avg_generic:.2f} compression ratio")
        print(f"   {chosen_algo} CABAC: {avg_specialized:.2f} compression ratio")
        print(f"   Improvement:       {avg_improvement:+.1f}%")
        
        # Context analysis
        print(f"\nüî¨ CONTEXT USAGE:")
        print(f"   Generic used: {sum(1 for ctx in generic_cabac.context_models.values() if ctx.usage_count > 0)} contexts")
        print(f"   {chosen_algo} used: {sum(1 for ctx in specialized_cabac.context_models.values() if ctx.usage_count > 0)} contexts")

if __name__ == "__main__":
    main()